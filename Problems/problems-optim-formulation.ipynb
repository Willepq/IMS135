{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import math as m\n",
    "import os\n",
    "from matplotlib.image import imread   #beh√∂bdes \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams # for changing default values\n",
    "import scipy.io as sio\n",
    "from scipy.optimize import minimize\n",
    "import timeit\n",
    "import torch\n",
    "import random\n",
    "\n",
    "#For installation besides anaconda, vscode I needed to:\n",
    "#In anaconda prompt\n",
    "#conda activate base \n",
    "#conda install pytorch\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1:\n",
    "\n",
    "Find the minimum value of $f(x)=(x-3)^2$ by writing your own steepest descent code. Assume the initial guess $x=1$. \n",
    "\n",
    "* Assume learning rate $\\alpha=1.$, $0.75$, $0.5$, $0.25$. If convergence criteria is $ |df/dx|<1e-3 $, how many iterations are neeeded for the different $\\alpha$? Does all values on $\\alpha$ manage to converge?\n",
    "\n",
    "* If you instead adopt the full Newton solver. How many iterations are needed until the same convergence criteria is fulfilled? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Specific options for scipy optimizers (see https://docs.scipy.org/doc/scipy/tutorial/optimize.html) see\n",
    "\n",
    "*Nelder-Mead simplex: https://docs.scipy.org/doc/scipy/reference/optimize.minimize-neldermead.html#optimize-minimize-neldermead\n",
    "\n",
    "* L-BFGS: https://docs.scipy.org/doc/scipy/reference/optimize.minimize-lbfgsb.html#optimize-minimize-lbfgsb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with alpha = 0.01, x = 2.999514517945703, nr of iterations: 412\n",
      "with alpha = 0.25, x = 2.999755859375, nr of iterations: 13\n",
      "with alpha = 0.5, x = 3.0, nr of iterations: 2\n",
      "no convergence\n",
      "with alpha = 1.0, x = 1.0, nr of iterations: 10000\n"
     ]
    }
   ],
   "source": [
    "# Problem 1\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return (x-3)**2\n",
    "\n",
    "\n",
    "def dfdx(x_range):\n",
    "    grads = []\n",
    "    for vals in x_range:\n",
    "        x = torch.tensor(vals, requires_grad=True)\n",
    "        y = f(x)\n",
    "        y.backward()\n",
    "        grads.append(x.grad.item())\n",
    "\n",
    "    return np.array(grads)\n",
    "\n",
    "\n",
    "def steepest_descent(alf, x0, tol):\n",
    "    xk = x0\n",
    "    max_iter = 10000\n",
    "\n",
    "    for iter in range(1,max_iter+1):\n",
    "        g = 2*(xk-3)\n",
    "        xk1 = xk - alf*g\n",
    "\n",
    "        if abs(g) <= tol:\n",
    "            return xk1, iter\n",
    "        elif iter == max_iter:\n",
    "            print('no convergence')\n",
    "            return xk1, iter\n",
    "        \n",
    "        xk = xk1\n",
    "\n",
    "\n",
    "x_vals = np.linspace(-10, 10)\n",
    "alf_range = np.array([0.01, 0.25, 0.5, 1])\n",
    "\n",
    "plt.figure(1)\n",
    "\n",
    "plt.plot(x_vals, f(x_vals), label='f(x)')\n",
    "plt.plot(x_vals, dfdx(x_vals), label = \"f'(x)\")\n",
    "\n",
    "for alf in alf_range:\n",
    "    xk, iter = steepest_descent(alf,1,1e-3)\n",
    "    plt.plot(xk, f(xk), marker='x', markersize=10, label=f'alpha = {alf}')\n",
    "    print(f'with alpha = {alf}, x = {xk}, nr of iterations: {iter}')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Rosenbrock function\n",
    "Find the minimum to Rosenbrock's function $(a - x_1)^2 + b \\,  (x_2 - x_1^2)^2$ with $a=1$ and $b=10$ by\n",
    "\n",
    "a) using SciPy Nelder-Mead and BFGS. Starting guess $x_1=1.3$, $x_2=0.7$. Do both algorithms find the minimum? If not try with a better starting guess.\n",
    "\n",
    "b) your own gradient descent solver. Investigate if you can find the solution by tuning the learning rate. How many iterations does it take to obtain convergence? Use tolerance $\\sqrt{\\nabla f \\cdot \\nabla f}<10^{-3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2\n",
    "\n",
    "# Enable external window plotting if running in Jupyter\n",
    "try:\n",
    "    get_ipython\n",
    "    %matplotlib qt\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "def Rosenbrocks_fun(x):\n",
    "    x1,x2 = x\n",
    "    return(1 - x1)**2 + 10*(x2 - x1**2)**2\n",
    "\n",
    "# Create a grid of x1, x2 values\n",
    "x1 = np.linspace(-2, 2, 200)\n",
    "x2 = np.linspace(-1, 3, 200)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "x = np.array([X1,X2])\n",
    "\n",
    "# Evaluate function on the grid\n",
    "Z = Rosenbrocks_fun(x)\n",
    "\n",
    "\n",
    "# Minimize function\n",
    "x0 = np.array([1.3, 0.7])\n",
    "res = minimize(Rosenbrocks_fun, x0, method='Nelder-Mead')\n",
    "Xmin = res.x\n",
    "fun_min = res.fun\n",
    "\n",
    "# Make 3D surface plot\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Plot surface\n",
    "surf = ax.plot_surface(X1, X2, Z, cmap=\"viridis\", edgecolor=\"none\", alpha=0.8)\n",
    "\n",
    "# Plot minimum point\n",
    "ax.scatter(Xmin[0], Xmin[1], fun_min, color='red', marker='x', s=100, label=\"minimum\")\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(\"x1\")\n",
    "ax.set_ylabel(\"x2\")\n",
    "ax.set_zlabel(\"f(x1, x2)\")\n",
    "ax.set_title(\"Rosenbrock's Function\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "For the experimental data set found in rubber_data.mat find the material parameter values of the Yeoh model that gives the best fit. The Yeoh model is defined as\n",
    "$$ \\sigma=c_1 \\, (\\lambda^2-1)+4 \\, c_2 \\lambda^2 (\\lambda^2+2/\\lambda-3)+6 \\, c_3 \\, \\lambda \\, (\\lambda^2+2/\\lambda-3)^2 $$\n",
    "where $\\lambda=\\epsilon+1$ and the material parameters are $c_1$-$c_3$\n",
    "\n",
    "a) Determine the parameter  values by adopting a cross-validation strategy. Withhold approx 10% of the data, validate against 10% of data, and train with 80% of the data. Use linear regression (pseudo-inverse).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat, whosmat\n",
    "\n",
    "# print(whosmat(\"problemData/rubber_data.mat\"))\n",
    "mat = loadmat(\"problemData/rubber_data.mat\", squeeze_me=True)\n",
    "\n",
    "stress_data = np.array(mat['stress'])\n",
    "strain_data = np.array(mat['strain'])\n",
    "\n",
    "#use code from Python help file\n",
    "all_combinations = []\n",
    "def generate_combinations(data_points, leave_out_count, start, comb=[]):\n",
    "    if len(comb) == len(data_points) - leave_out_count:\n",
    "        all_combinations.append(comb)\n",
    "        return\n",
    "    for i in range(start, len(data_points)):\n",
    "        generate_combinations(data_points, leave_out_count,i + 1, comb + [data_points[i]])\n",
    "\n",
    "\n",
    "def left_out_points_fcn(data_points,leave_out_count):\n",
    "    generate_combinations(data_points, leave_out_count,0)\n",
    "# Iterate through combinations and leave out  data points\n",
    "    left_out_points=[]\n",
    "    for combination in all_combinations:\n",
    "        left_out_points.append(np.setdiff1d(data_points, combination))\n",
    "    return np.array(left_out_points),np.array(all_combinations)\n",
    "\n",
    "###### end from Python help file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "b) Extend the Yeoh model by adding the terms $c_4 \\, (\\lambda-1)^6+c_5 \\, (e^{-c_6 \\, \\lambda}-1)$. Use the cross-validation approach in a) again to find the parameter values. Use SciPy 'L-BFGS-B', with 'gtol=1.e-5 and  $x0 = [5.,-0.5,0.1,0.,0.,0.]$. \n",
    "\n",
    "c) Add L1 (LASSO) regularization on the added parameters ($c_4$-$c_6$) and solve b) again with different regularization parameters. Show how the results depend on the choice of regularization parameter. Try penalty 0.0001, 1, 10. se SciPy 'L-BFGS-B', with 'gtol=1.e-5 and  $x0 = [5.,-0.5,0.1,0.,0.,0.]$. For what of the values of the penalty parameter do we approach the parameter values found in a)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "Minimize $f(x_1,x_2)=1/4 \\, (f_1(x_1,x_2)+f_2(x_1,x_2)+f_3(x_1,x_2)+f_4(x_1,x_2))$ where\n",
    "$$f_1=(x_1-1)^2+(x_2-1)^2$$\n",
    "$$f_2=(x_1+1)^2+0.5 \\, (x_2-1)^2$$\n",
    "$$f_3=0.7 \\, (x_1+1)^2+0.5 \\, (x_2+1)^2$$\n",
    "$$f_4=0.7 \\, (x_1-1)^2+0.5 \\, (x_2+1)^2$$\n",
    "Use starting values $x_1=-1$ and $x_2=2$.\n",
    "\n",
    "a) Implement the stochastic gradient descent to find the minimum. Assume a constant learning rate $\\alpha=0.4$. Visualize the objective function value during the first 20 iterations.\n",
    "\n",
    "b) Now adopt a decaying learning rate $\\alpha_0=0.4$, ..., $\\alpha_k=0.4/\\sqrt{1+k}$, ... Visualize the objective function value during the first 20 iterations.\n",
    "\n",
    "c) Try to find the solution by using optimization with Adagrad and LBFGS in Pytorch (see example in python_help.ipynb) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
